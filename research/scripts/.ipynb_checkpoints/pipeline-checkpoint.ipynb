{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bee53e3e-2ce6-4b23-9bc3-34ff2a2a0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "def generate_generators(data_root, hdf_data_path,BZ,IR,\n",
    "                        label_scheme='label'):\n",
    "    \"\"\"\n",
    "    Gathers all files and organize into train/validation/test. Each\n",
    "       data subset is further organized by class. Converts file lists\n",
    "       into iterable data generators.\n",
    "    Inputs:\n",
    "        * data_root: full path to the data root containing subdirectories of data\n",
    "        * trn_list, val_list, tst_list: each is a list of strings. The strings\n",
    "            should encode the strata being used to divide data into segments and\n",
    "            must match the corresponding field of the FileName class.\n",
    "        * hdf_data_path: the path used internally in the HDF to get the desired surface\n",
    "        * batch_size: the _approximate_ number of samples to be used __next__ call. Note\n",
    "              that if batch_size // n_classes is non-integer then it may be slightly off.\n",
    "        * label_scheme: a string that matches the name of the property of the FileName\n",
    "              class that you want to use as the label.\n",
    "        * strata_scheme: a string that matches the name of the property of the FileName\n",
    "              class that you want to compare against trn_list (etc.) for data segmentation.\n",
    "    \"\"\"\n",
    "    n_classes = 2\n",
    "    tst_files = [[] for clas in range(n_classes)]\n",
    "    trn_files = [[] for clas in range(n_classes)]\n",
    "\n",
    "    files= [tst_files, trn_files]\n",
    "    #print('Allocating HDFs to train/valid/test...')\n",
    "    for filename in os.listdir(data_root):\n",
    "        if not filename.endswith('.hdf'):\n",
    "            continue\n",
    "\n",
    "        # Extract the label using 'label_scheme' identifier\n",
    "        label = int(int(filename.split('_')[3]) > 0)  # 0 = clutter (not manmade); 1 = target (manmade)\n",
    "\n",
    "        # list that stores files in two sub-lists\n",
    "        randr = random.randrange(0,5)\n",
    "        if randr < 1:\n",
    "            \n",
    "            files[0][label].append(filename)\n",
    "\n",
    "        else:\n",
    "\n",
    "            files[1][label].append(filename)\n",
    "\n",
    "    # Wrap each file list into an iterable data generator that actually\n",
    "    #     read the HDFs when __next__ is called:\n",
    "    \n",
    "    trn_gen = DataGenerator(data_root, trn_files, hdf_data_path,  BZ, IR)\n",
    "    tst_gen = DataGenerator(data_root, tst_files, hdf_data_path, BZ, IR)\n",
    "    length = len(trn_files[0]) #+ len(trn_files[1])\n",
    "\n",
    "    return trn_gen, tst_gen\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, data_root, file_list, hdf_data_path, BZ, IR, n_classes=2):\n",
    "        # Basic properties:\n",
    "        self.data_root = data_root\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        \"balance by class\"\n",
    "        self.hdf_path = hdf_data_path\n",
    "\n",
    "        self._permanent_file_list = file_list\n",
    "        # print(len(file_list[0]), \"\\n\")\n",
    "        \"for august-november split\"\n",
    "        # self.target_list = file_list[1][0:4\n",
    "        \"for fulll dataset\"\n",
    "        self.target_list = file_list[1]\n",
    "        self.target_len = len(self.target_list)\n",
    "        print(self.target_len, 'length of target')\n",
    "        \n",
    "        \"test\"\n",
    "\n",
    "        #self.clutter_list = file_list[0]\n",
    "        self.clutter_list = file_list[0][0:int(IR*self.target_len)]\n",
    "\n",
    "        #self.clutter_list = file_list[0][0:16799]\n",
    "        self.clutter_len = len(self.clutter_list)\n",
    "        print(self.clutter_len, 'length of clutter')\n",
    "\n",
    "        self.dataset_size = self.clutter_len + self.target_len\n",
    "        self.batch_size = BZ\n",
    "        self.bsz_by_class = int(self.batch_size / self.n_classes)\n",
    "\n",
    "        if self.batch_size % 2 != 0 or self.bsz_by_class % 2 != 0:\n",
    "            print('batch size is odd or not balanced... \\nadding one to batch size')\n",
    "            self.batch_size = self.batch_size  + 1\n",
    "            self.bsz_by_class = math.floor(self.batch_size / self.n_classes)\n",
    "\n",
    "        n = 1\n",
    "        while n < 6:\n",
    "            if  self.bsz_by_class % 2 != 0:\n",
    "                print('batch size is not balanced... \\nadding one to batch size')\n",
    "                self.batch_size = self.batch_size  + 1\n",
    "                self.bsz_by_class = math.floor(self.batch_size / self.n_classes)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        print(self.batch_size, 'batch_size')\n",
    "        print(self.bsz_by_class, 'balance size')\n",
    "\n",
    "        #self.current_index = [i for i in range(self.batch_size)]\n",
    "\n",
    "\n",
    "        #self.current_index = [0 for label in range(self.n_classes)]\n",
    "\n",
    "        self.HDF_n_rows = 71\n",
    "        self.HDF_n_cols = 71\n",
    "        self.HDF_n_dpth = 101\n",
    "\n",
    "        self.input_shape = (self.batch_size, self.HDF_n_rows, self.HDF_n_cols, self.HDF_n_dpth)\n",
    "\n",
    "        self.chip_n_rows = 64\n",
    "        self.chip_n_cols = 64\n",
    "        self.chip_n_dpth = 101\n",
    "\n",
    "        \"chip shape is one cube from batch\"\n",
    "        self.chip_shape = (self.chip_n_rows, self.chip_n_cols, self.chip_n_dpth)\n",
    "\n",
    "        self.batch_shape = (self.batch_size, self.chip_n_rows, self.chip_n_cols, self.chip_n_dpth)\n",
    "\n",
    "\n",
    "        # Make a boolean indexing mask for efficient extraction of the chip center:\n",
    "        \"input shape minus chip shape row\"\n",
    "        row_diff = self.input_shape[1] - self.chip_shape[0]\n",
    "        \"this is chopping off one half the difference on each side\"\n",
    "        first_row = row_diff // 2\n",
    "        last_row = first_row + self.chip_shape[0]\n",
    "\n",
    "        \"this is chopping off one half the difference on each side\"\n",
    "        col_diff = self.input_shape[2] - self.chip_shape[1]\n",
    "        first_col = col_diff // 2\n",
    "        last_col = first_col + self.chip_shape[1]\n",
    "\n",
    "        \"this is chopping off one half the difference on each side\"\n",
    "        slice_diff = self.input_shape[3] - self.chip_shape[2]\n",
    "        first_slice = slice_diff // 2\n",
    "        last_slice = first_slice + self.chip_shape[2]\n",
    "\n",
    "        \"slicing out the chips from the input data\"\n",
    "        \"list with shape of input all False values\"\n",
    "        self.center_select = np.full((self.HDF_n_rows, self.HDF_n_cols, self.HDF_n_dpth), False)\n",
    "        \"except the chips size all true\"\n",
    "        self.center_select[first_row:last_row, first_col:last_col, first_slice:last_slice] = True\n",
    "\n",
    "        # self.first_slice = 0\n",
    "        # self.last_slice = self.bsz_by_class\n",
    "\n",
    "\n",
    "    def readHDF(self, file_name):\n",
    "        \"\"\" Reads data from the HDF\"\"\"\n",
    "        with h5py.File(os.path.join(self.data_root, file_name), mode='r') as f:\n",
    "            data = f[self.hdf_path][:]\n",
    "            data = np.transpose(data)  # because python reverses the order of the 3d volume, this will correct back to the original matlab order\n",
    "            #data = data / 40  # Now data is in [0,1], to match the 2d sensors\n",
    "\n",
    "\n",
    "        return data\n",
    "\n",
    "    def chip_center(self, data):\n",
    "        \"\"\" extracts the center of the chip via boolean indexing. \"\"\"\n",
    "        return np.reshape(data[self.center_select], self.chip_shape)\n",
    "\n",
    "\n",
    "    def preprocess(self, data_sample):\n",
    "        \"\"\"\n",
    "        Preprocesses a data sample.\n",
    "        TODO: use configuration file and preprocesser class like ADAM dataloader here.\n",
    "        \"\"\"\n",
    "        # TODO: sometimes want to jiggle the chip instead of centering so will\n",
    "        #          need to remove chip_center from here.\n",
    "\n",
    "        \"centering x cube\"\n",
    "        x_center = self.chip_center(data_sample)\n",
    "        return x_center\n",
    "\n",
    "    def perm_target_list(self, target_list):\n",
    "        if self.last_slice % self.target_len == 0:\n",
    "            print('target list permuted')\n",
    "            return np.random.permutation(target_list)\n",
    "        else:\n",
    "            return target_list\n",
    "        #return np.random.permutation(target_list)\n",
    "\n",
    "    def reset_list(self, target_list):\n",
    "\n",
    "        # Come up with a way to reset_list\n",
    "\n",
    "\n",
    "            return target_list\n",
    "\n",
    "    def data_loop(self, list_data):\n",
    "\n",
    "        \"place holder for batch of data\"\n",
    "        batch_data = np.zeros(self.batch_shape, dtype='float32')\n",
    "        \"batch labels\"\n",
    "        batch_label = np.zeros(self.batch_size)\n",
    "\n",
    "        for label in range(0, self.n_classes):\n",
    "            for nth_sample in range(0, self.bsz_by_class):\n",
    "\n",
    "                ld = list_data[label]\n",
    "                sample = ld[nth_sample]\n",
    "\n",
    "                data = self.readHDF(sample)\n",
    "\n",
    "                # Preprocessing can go here, e.g. random flips/translations/normalizations\n",
    "                \"centers the data here\"\n",
    "                data = self.preprocess(data)\n",
    "\n",
    "                # Insert the data into the batch_data and batch_label arrays:\n",
    "                batch_idx = self.bsz_by_class * label + nth_sample\n",
    "                \"batch data: why just rows and columns\"\n",
    "                batch_data[batch_idx][:, :, :] = np.reshape(data, self.chip_shape)\n",
    "                \"batch label\"\n",
    "                batch_label[batch_idx] = label\n",
    "\n",
    "        return batch_data, batch_label\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.first_slice = 0\n",
    "        self.last_slice = self.bsz_by_class\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(label_set) for label_set in self.target_list])\n",
    "\n",
    "    def __next__(self):\n",
    "\n",
    "\n",
    "        # Put `bsz_by_class' samples from each class into `batch_data':\n",
    "\n",
    "        # for n in range(self.bsz_by_class, self.target_len, self.bsz_by_class):\n",
    "\n",
    "\n",
    "        if self.last_slice <= self.clutter_len:\n",
    "            #print(self.last_slice, 'last slice')\n",
    "\n",
    "            if self.last_slice > self.target_len:\n",
    "                tlp = self.perm_target_list(self.target_list)\n",
    "                tl = self.reset_list(tlp)\n",
    "\n",
    "                cl = self.clutter_list[self.first_slice:self.last_slice]\n",
    "\n",
    "                list_data = [cl, tl]\n",
    "\n",
    "                batch_data, batch_label = self.data_loop(list_data)\n",
    "\n",
    "            elif self.last_slice <= self.target_len :\n",
    "                tl = self.target_list[self.first_slice:self.last_slice]\n",
    "                cl = self.clutter_list[self.first_slice:self.last_slice]\n",
    "\n",
    "\n",
    "                list_data = [cl, tl]\n",
    "\n",
    "\n",
    "                batch_data, batch_label = self.data_loop(list_data)\n",
    "\n",
    "        else:\n",
    "            print('next epoch')\n",
    "            raise StopIteration\n",
    "\n",
    "        self.first_slice += 1\n",
    "        self.last_slice += 1\n",
    "        return batch_data, batch_label\n",
    "        # , self.bsz_by_class, self.clutter_len, self.target_len, self.batch_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d6e135c-556d-4d71-831a-b7394dcc7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    _relu = nn.ReLU()\n",
    "    \n",
    "    return _relu(x)\n",
    "# change to global min / max\n",
    "def curly_N(w):\n",
    "    w_min, w_max = torch.min(torch.min(torch.min(w))), torch.max(torch.max(torch.max(w)))\n",
    "    reg_N = (w - w_min) / (w_max - w_min)\n",
    "    return reg_N\n",
    "\n",
    "def curly_Nprime(w):\n",
    "    w_min, w_max = torch.min(torch.min(torch.min(w))), torch.max(torch.max(torch.max(w)))\n",
    "    curly_N = (w - w_min + 1) / (w_max - w_min + 2)\n",
    "    return curly_N\n",
    "    # return (w - torch.min(w) + 1) / (torch.max(w) - torch.min(w) + 2)\n",
    "\n",
    "def f_VHN(x, w):\n",
    "    relu_x = relu(curly_N(x))\n",
    "    relu_w = relu(curly_Nprime(w))\n",
    "    \n",
    "    return relu_x * relu_w\n",
    "    \n",
    "class VHNLayer(nn.Module):\n",
    "    \"\"\" Custom VHN layer \"\"\"\n",
    "    def __init__(self, channels, img_len, img_width):\n",
    "        super().__init__()\n",
    "        self.channels, self.img_len, self.img_width = channels, img_len, img_width\n",
    "        weights = torch.Tensor(channels, img_len, img_width)\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\n",
    "        # initialize weights and biases\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return f_VHN(x, self.weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2f9c03f-1b56-4a4e-9bc0-a40df78e8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from load_data import device\n",
    "\"fix dimensions\"\n",
    "\"may need \"\n",
    "import matplotlib.pyplot as plt\n",
    "# from colormap import *\n",
    "import os\n",
    "\n",
    "class ATR(nn.Module):\n",
    "    def __init__(self, nc, device = None, N_filters=4, N_output = 1, ker = 4, s = 2, pad =1):\n",
    "        super(ATR, self).__init__()\n",
    "        self.ker = ker\n",
    "        self.s = s\n",
    "        self.pad = pad\n",
    "        self.nc = nc\n",
    "        # self.device = device\n",
    "        self.N_filters = N_filters\n",
    "        self.N_output = N_output\n",
    "        self.vhn = VHNLayer(101, 64, 64)\n",
    "        self.conv1 = nn.Conv3d(nc, N_filters, kernel_size = (3, 3, 3), stride=(1, 2, 2), padding= (0, 1, 1))\n",
    "        self.conv2 = nn.Conv3d(N_filters, N_filters, kernel_size = (3, 3, 3), stride=(1, 2, 2), padding= (0, 1, 1))\n",
    "        self.conv3 = nn.Conv3d(N_filters, N_filters, kernel_size = (3, 3, 3), stride=(1, 2, 2), padding= (0, 1, 1))\n",
    "        self.conv4 = nn.Conv3d(N_filters, 1, kernel_size = (3, 3, 3), stride=(1, 2, 2), padding= (0, 1, 1))\n",
    "        self.avgpool = nn.AvgPool3d(kernel_size = (6, 2, 2), stride= (1, 1, 1), padding= (0, 0, 0))\n",
    "    \n",
    "\n",
    "        # \"columns input x and output columnes\"\n",
    "\n",
    "        self.f2 = nn.Linear(1248,1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x0):\n",
    "        \"image vectorization\"\n",
    "        # print(x0.shape)\n",
    "        x0 = self.vhn.forward(x0)\n",
    "        x = self.conv1(x0.unsqueeze(1))\n",
    "        x = self.avgpool(F.relu(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.avgpool(F.relu(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.avgpool(F.relu(x))\n",
    "        x = self.conv4(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "\n",
    "        x = self.f2(x)\n",
    "\n",
    "        y = self.sigmoid(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2205becb-221e-46d0-a01a-dfc301b8506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ATR(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6b5e92e-1c13-48da-be4f-6722fc586f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac5ef9b3-faae-4ec7-99e6-8fdf56c303cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be18efb3-1940-4f57-9122-576a26dde859",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../research_data/sas_full_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d7124b66-b2c2-4d7f-b64d-64fef384d5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412 length of target\n",
      "412 length of clutter\n",
      "20 batch_size\n",
      "10 balance size\n",
      "88 length of target\n",
      "88 length of clutter\n",
      "20 batch_size\n",
      "10 balance size\n"
     ]
    }
   ],
   "source": [
    "dldr_trn, dldr_tst = generate_generators(data_root = path , hdf_data_path = 'DL_info/chip_info/cube_raw',BZ = 20, IR = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20745286-1e42-4fc4-ad2b-d1f4fa8f949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "[1,     5] loss: 0.041\n",
      "[1,    10] loss: 0.042\n",
      "[1,    15] loss: 0.039\n",
      "[1,    20] loss: 0.043\n",
      "[1,    25] loss: 0.040\n",
      "[1,    30] loss: 0.034\n",
      "[1,    35] loss: 0.033\n",
      "[1,    40] loss: 0.038\n",
      "[1,    45] loss: 0.035\n",
      "[1,    50] loss: 0.035\n",
      "[1,    55] loss: 0.042\n",
      "[1,    60] loss: 0.042\n",
      "[1,    65] loss: 0.031\n",
      "[1,    70] loss: 0.026\n",
      "[1,    75] loss: 0.032\n",
      "[1,    80] loss: 0.036\n",
      "[1,    85] loss: 0.042\n",
      "[1,    90] loss: 0.042\n",
      "[1,    95] loss: 0.039\n",
      "[1,   100] loss: 0.033\n",
      "[1,   105] loss: 0.029\n",
      "[1,   110] loss: 0.027\n",
      "[1,   115] loss: 0.023\n",
      "[1,   120] loss: 0.025\n",
      "[1,   125] loss: 0.026\n",
      "[1,   130] loss: 0.029\n",
      "[1,   135] loss: 0.030\n",
      "[1,   140] loss: 0.027\n",
      "[1,   145] loss: 0.029\n",
      "[1,   150] loss: 0.034\n",
      "[1,   155] loss: 0.030\n",
      "[1,   160] loss: 0.027\n",
      "[1,   165] loss: 0.026\n",
      "[1,   170] loss: 0.025\n",
      "[1,   175] loss: 0.028\n",
      "[1,   180] loss: 0.030\n",
      "[1,   185] loss: 0.031\n",
      "[1,   190] loss: 0.031\n",
      "[1,   195] loss: 0.031\n",
      "[1,   200] loss: 0.022\n",
      "[1,   205] loss: 0.019\n",
      "[1,   210] loss: 0.025\n",
      "[1,   215] loss: 0.029\n",
      "[1,   220] loss: 0.026\n",
      "[1,   225] loss: 0.022\n",
      "[1,   230] loss: 0.027\n",
      "[1,   235] loss: 0.029\n",
      "[1,   240] loss: 0.024\n",
      "[1,   245] loss: 0.020\n",
      "[1,   250] loss: 0.021\n",
      "[1,   255] loss: 0.028\n",
      "[1,   260] loss: 0.022\n",
      "[1,   265] loss: 0.020\n",
      "[1,   270] loss: 0.029\n",
      "[1,   275] loss: 0.039\n",
      "[1,   280] loss: 0.033\n",
      "[1,   285] loss: 0.019\n",
      "[1,   290] loss: 0.021\n",
      "[1,   295] loss: 0.026\n",
      "[1,   300] loss: 0.018\n",
      "[1,   305] loss: 0.016\n",
      "[1,   310] loss: 0.024\n",
      "[1,   315] loss: 0.024\n",
      "[1,   320] loss: 0.024\n",
      "[1,   325] loss: 0.023\n",
      "[1,   330] loss: 0.021\n",
      "[1,   335] loss: 0.022\n",
      "[1,   340] loss: 0.022\n",
      "[1,   345] loss: 0.023\n",
      "[1,   350] loss: 0.019\n",
      "[1,   355] loss: 0.023\n",
      "[1,   360] loss: 0.024\n",
      "[1,   365] loss: 0.022\n",
      "[1,   370] loss: 0.015\n",
      "[1,   375] loss: 0.024\n",
      "[1,   380] loss: 0.020\n",
      "[1,   385] loss: 0.014\n",
      "[1,   390] loss: 0.019\n",
      "next epoch\n",
      "epoch 1\n",
      "[2,     5] loss: 0.017\n",
      "[2,    10] loss: 0.018\n",
      "[2,    15] loss: 0.017\n",
      "[2,    20] loss: 0.020\n",
      "[2,    25] loss: 0.017\n",
      "[2,    30] loss: 0.015\n",
      "[2,    35] loss: 0.016\n",
      "[2,    40] loss: 0.020\n",
      "[2,    45] loss: 0.015\n",
      "[2,    50] loss: 0.016\n",
      "[2,    55] loss: 0.020\n",
      "[2,    60] loss: 0.021\n",
      "[2,    65] loss: 0.015\n",
      "[2,    70] loss: 0.011\n",
      "[2,    75] loss: 0.016\n",
      "[2,    80] loss: 0.019\n",
      "[2,    85] loss: 0.024\n",
      "[2,    90] loss: 0.025\n",
      "[2,    95] loss: 0.021\n",
      "[2,   100] loss: 0.018\n",
      "[2,   105] loss: 0.014\n",
      "[2,   110] loss: 0.013\n",
      "[2,   115] loss: 0.012\n",
      "[2,   120] loss: 0.015\n",
      "[2,   125] loss: 0.012\n",
      "[2,   130] loss: 0.014\n",
      "[2,   135] loss: 0.016\n",
      "[2,   140] loss: 0.014\n",
      "[2,   145] loss: 0.014\n",
      "[2,   150] loss: 0.019\n",
      "[2,   155] loss: 0.017\n",
      "[2,   160] loss: 0.014\n",
      "[2,   165] loss: 0.013\n",
      "[2,   170] loss: 0.013\n",
      "[2,   175] loss: 0.016\n",
      "[2,   180] loss: 0.017\n",
      "[2,   185] loss: 0.018\n",
      "[2,   190] loss: 0.018\n",
      "[2,   195] loss: 0.019\n",
      "[2,   200] loss: 0.013\n",
      "[2,   205] loss: 0.010\n",
      "[2,   210] loss: 0.014\n",
      "[2,   215] loss: 0.017\n",
      "[2,   220] loss: 0.015\n",
      "[2,   225] loss: 0.013\n",
      "[2,   230] loss: 0.017\n",
      "[2,   235] loss: 0.018\n",
      "[2,   240] loss: 0.014\n",
      "[2,   245] loss: 0.012\n",
      "[2,   250] loss: 0.011\n",
      "[2,   255] loss: 0.018\n",
      "[2,   260] loss: 0.013\n",
      "[2,   265] loss: 0.012\n",
      "[2,   270] loss: 0.018\n",
      "[2,   275] loss: 0.027\n",
      "[2,   280] loss: 0.023\n",
      "[2,   285] loss: 0.011\n",
      "[2,   290] loss: 0.013\n",
      "[2,   295] loss: 0.016\n",
      "[2,   300] loss: 0.011\n",
      "[2,   305] loss: 0.011\n",
      "[2,   310] loss: 0.017\n",
      "[2,   315] loss: 0.017\n",
      "[2,   320] loss: 0.015\n",
      "[2,   325] loss: 0.015\n",
      "[2,   330] loss: 0.013\n",
      "[2,   335] loss: 0.014\n",
      "[2,   340] loss: 0.015\n",
      "[2,   345] loss: 0.017\n",
      "[2,   350] loss: 0.014\n",
      "[2,   355] loss: 0.018\n",
      "[2,   360] loss: 0.018\n",
      "[2,   365] loss: 0.017\n",
      "[2,   370] loss: 0.009\n",
      "[2,   375] loss: 0.017\n",
      "[2,   380] loss: 0.014\n",
      "[2,   385] loss: 0.008\n",
      "[2,   390] loss: 0.012\n",
      "next epoch\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    print(\"epoch {epoch}\".format(epoch=epoch))\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dldr, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "    \n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = torch.log10(torch.tensor(inputs).transpose(1,3) + 1)\n",
    "        # print(inputs.shape)\n",
    "        labels = torch.tensor(labels)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(inputs)\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss1 = criterion1(outputs, labels.reshape(20,1).type(torch.float32))\n",
    "        \n",
    "        loss2 = criterion2(curly_Nprime(net.vhn.weights), curly_N(torch.sum(inputs, dim = 0) / dldr.batch_size))\n",
    "        # print(\"netvhn\", net.vhn.weights.shape)\n",
    "        # print(curly_N(torch.sum(inputs, dim = 0) / dldr.batch_size).shape)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 5 == 4:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 5:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff0b6b82-894d-40af-add8-6a0dbf28643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRAUC\u001b[39m\u001b[38;5;124m\"\u001b[39m, binary_auprc(torch\u001b[38;5;241m.\u001b[39mtensor(preds), torch\u001b[38;5;241m.\u001b[39mtensor(labels), num_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(preds)))\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "import torcheval\n",
    "from torcheval.metrics.functional import binary_auprc\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dldr_tst,0):\n",
    "        inputs, label = data\n",
    "        \n",
    "        inputs = torch.log10(torch.tensor(inputs).transpose(1,3) + 1)\n",
    "        # print(inputs.shape)\n",
    "        label = torch.tensor(label)\n",
    "        # calculate outputs by running images through the network\n",
    "        output = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        preds.append(output)\n",
    "        labels.append(label)\n",
    "        print(i)\n",
    "\n",
    "        if i > 2:\n",
    "            break\n",
    "\n",
    "print(\"PRAUC\", binary_auprc(torch.tensor(preds), torch.tensor(labels), num_tasks=len(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f412e-c8e4-4315-a7ac-8f4ee24cb505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61831703-54ee-4b10-8a58-12bf98a101a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
